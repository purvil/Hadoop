{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop: Definition\n",
    "* Hadoop is an open source software platform, for **distributed storage** and **distributed processing** of very large dataset on computer clusters built from **commodity hardware**.\n",
    "* Mapreduce is batch processing system and NOT suitable for interactive analysis. It is capable of running query on large dataset and get result in reasonable time.\n",
    "* Open source project, based on Google's MapReduce and GFS (Google File System).\n",
    "* Hadoop is Not good for,\n",
    "    - Random access (Process transaction)\n",
    "    - When work can not parallelized.\n",
    "    - Not good for processing lots of small files\n",
    "    - Not good for low latency data access\n",
    "    - Not good for intensive calculation on little data.\n",
    "* Node is simply a computer. Rack is collection of nodes that are physically located near by and all connected to same network switch. Cluster is collection of racks.\n",
    "* Hadoop used to have 2 main component:\n",
    "    - Distributed File System : HDFS\n",
    "    - Map Reduce Engine:\n",
    "        - Framework for performing calculation on data in file system. In Hadoop 1.0 it also act as resource manager and scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with other systems\n",
    "\n",
    "* **Iterative processing**: ML algo are iterative in nature, so it is efficient to hold intermediate result in memory, compare to loading from disk each time. MapReduce does not allows this, Spark does.\n",
    "\n",
    "* **Stream processing**: Storm, spark streaming allows us to run real time distributed computations on unbounded stream of data and emits result to Hadoop storage or external system.\n",
    "\n",
    "* **Search** : Solr search can run on Hadoop clusters, indexing document as they are loaded to HDFS and serving search queries from indexes stored in HDFS.\n",
    "\n",
    "#### Relational database management systems and MapReduce\n",
    "* RDBMS is good for updating small portion of database B-tree works well. Because it is limited by seek time (Moving disk head to particular place at disk where we want to read or write data.) it is not suitable to update large amount of data. Mapreduce is good fit for that as it uses Sort/Merge to rebuild the database.\n",
    "* Mapreduce is good when we want to analyze large portion of dataset. RDBMS is good for point queries and update.\n",
    "* Mapreduce is good when data is written once and read many times, RDBMS is good where database is continuously updated.\n",
    "* Hadoop is good for semi and unstructured data as it interpret data at processing time (schema on read). Whereas RDBMS is costly in data loading phase as it required structured data (Schema on write).\n",
    "* Relational database is normalized for integrity and remove redundancy. In hadoop normalization make reading nonlocal operation.\n",
    "\n",
    "#### Grid computing/ Volunteer Computing and MapReduce/Hadoop\n",
    "* Grid computing is good for compute intensive task. We distribute work across cluster of machine, transferring data to cluster is limited by network bandwidth, so cluster can sit idle. Hadoop tries to co-locate the data with the compute nodes, so data access is fast because it is local. It is called data locality.\n",
    "* In grid computing manually we have to take care of recovery and checkpoints, failed job etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop based application\n",
    "\n",
    "### Avro:\n",
    "* Data structures within context of Hadoop MapReduce jobs.\n",
    "\n",
    "### Giraph :\n",
    "* Interactive graph processing using Hadoop\n",
    "\n",
    "### Mahout:\n",
    "* Framework for ML applications using Hadoop and spark\n",
    "\n",
    "### Pig:\n",
    "* Main components\n",
    "    - Pig Latin : High level data flow language to write scripts. Program by connecting thing together. Ex. make relation LOAD, STORE etc.\n",
    "    - Infrastructure layer: Translate pig latin to MR code.\n",
    "* In built operators and functions. Used to Extract, Transform, Load operations. Manipulating and analyzing raw data.\n",
    "* Pig latin, lets SQL like syntax to define map and reduce.. Highly extensible with user defined function.\n",
    "* We can run using Grunt, script or Ambari/Hue. \n",
    "\n",
    "### HIVE\n",
    "* Query and manage data using HiveQL\n",
    "* Data warehouse software\n",
    "* HiveQL is SQL like language to structure, query data from HDFS or HBase.\n",
    "* Execution environment can be MapReduce, Tez, spark.\n",
    "\n",
    "### Impala\n",
    "* SQL to access data. Instead converting it to Map reduce like HIVE impala can access HDFS dierectly.\n",
    "* Low latency query, faster than HIVE. HIVE is more suitable for long batch processing jobs.\n",
    "\n",
    "### HBase\n",
    "* key-value store that uses HDFS for its underlying storage.\n",
    "* No-sql columnar database\n",
    "* Scalable data stores\n",
    "* Consistency, High availability: Range partition of keys across the server. Data is in HDFS so it gives you replication by default and automatic Sharding: Tables are distributed when they are large.\n",
    "* It provides online read/write access for individual rows and batch operations for reading and writing data in bulk.\n",
    "\n",
    "### Flume\n",
    "* Service for moving large amount of data around the cluster soon after data is produced.\n",
    "* Gathering log files from every machine in cluster\n",
    "* Transferring data to centralized persistent store (HDFS)\n",
    "\n",
    "### Sqoop\n",
    "* SQL to Hadoop\n",
    "* Transfer between Hadoop and relational database\n",
    "* Use mapreduce to import and export data.\n",
    "* Store data in HDFS as delimited file.\n",
    "\n",
    "### Oozie\n",
    "* Work flow scheduler\n",
    "* Way to schedule job in cluster.\n",
    "* Jobs is triggered by frequency or data availability.\n",
    "\n",
    "### Ambari : \n",
    "* Gives view of cluster, status of nodes\n",
    "* Lets you execute HIVE and PIG query too.\n",
    "\n",
    "### Hue\n",
    "* Similar to Ambari, which provides top level view to execute query on hadoop cluster.\n",
    "* Graphical front end to cluster\n",
    "\n",
    "### MESOS\n",
    "* Alternative to YARN\n",
    "* Resource negotiator.\n",
    "\n",
    "### STORM\n",
    "* Processing streaming data in real time\n",
    "\n",
    "### Zookeeper\n",
    "* Coordinate cluster.\n",
    "* Keeping track of which node is up or down\n",
    "* Maintain reliable and consistent state of cluster\n",
    "\n",
    "### Apache drill\n",
    "* Query engine. Allows to write sql query to query data from no-sql database.\n",
    "\n",
    "###  Apache phoenix\n",
    "* Allows to run SQL on hadoop cluster WITH ACID guarantee.\n",
    "\n",
    "### Presto\n",
    "\n",
    "### Zeppelin\n",
    "* Notebook lie approach to access cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop : Overview\n",
    "\n",
    "### HDFS:\n",
    "* Runs on top of existing file system on node. Design to tolerate high failure rate using replication of data.\n",
    "* Works best on large file as no need to frequent seek for data on disk.\n",
    "* Designed for sequential data access, as in such case at starting of each block seek is needed and after that it can read data in sequence. Random access is not efficient\n",
    "\n",
    "### HDFS file blocks:\n",
    "* It is not same as OS file blocks. Hadoop block = multiple OS block\n",
    "* Default size is 128 MB. \n",
    "* Blocks for single file are replicated on multiple nodes.\n",
    "* If chunk of file is smaller than block only needed space is used.\n",
    "* We can set replication factor in Hadoop configuration and even by each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make program parallel\n",
    "    - We divide input file to different processors and run in parallel\n",
    "    - Dividing is challenge, some machine finish processing early. So finish time is dominated by large file. Alternative is divide the file in fixed size chunk\n",
    "    - Combine the result. When we split by chunk data is divided to several chunk and combining them is hard.\n",
    "    - When we want to add distributed processing, we have to handle failure, monitor busy machines.\n",
    "    - Hadoop takes care of everything.\n",
    "    \n",
    "![](images/mapReduce.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing from/to HDFS\n",
    "### Reading from HDFS\n",
    "![](images/read.jpg)\n",
    "\n",
    "* Client open the file it wishes to read by calling open() on FileSystem object, HDFS is instance of DistributedFileSystem. DistributedFileSystem calls the namenode using RPC to find location of first few blocks. For each block namenode return address of datanode. datanode are sorted via proximity to client. Client also can be the other datanode. DistributedFileSystem Returns FSDataInputStream to the client for it to read data from. Client calls read() on stream. Client connected to first datanode for the first block of file.When end of block reach DFSInputStream will close the connection to datanode, then find best datanode for next block (client does not know about it). At last client call close() on FSDataInputStream.\n",
    "* DFSInputStream encounter error with datanode, it automatically switch to other replica and report the error to namenode.\n",
    "* As  client directly connected to datanode for data, we can have multiple client accessing different data nodes in parallel.\n",
    "\n",
    "### Distance between nodes\n",
    "- Process on same node\n",
    "- Different node in same rack\n",
    "- Nodes on different racks in the same data center\n",
    "- Nodes in different data center.\n",
    "![](images/distance.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDFS\n",
    "\n",
    "![](images/write.jpg)\n",
    "* Client create file by calling create() on DFS. DFS contact namenode via RPC, to create file in FS's namespace with no block. namenode check for duplicate file and permission for write. namenode make record of file. If fials client will get IOException. DFS return FSDataOutputStream for client to start wrting data to.FSDataOutputStream wraps DataStreamer which handle communication with datanode and namenode. data is splits in packet and written to data queue. It is cosumed by data streamer, which ask namenode to create new block by picking list of suitable datanode for replication. By default there are 3 node in pipeline. DataStreamer streams the packet to 1st datanode in the pipeline, which stores each packet and forward to the second datanode, second will forward to third.\n",
    "* DFSOutputStream maintain internal queue of packets that are waiting to be ack by datanodes, called ack queue. Packet is removed from ack queue only when  ack received from all replica datanode.\n",
    "* When client finish writing it calls close() on the stream.\n",
    "* Hadoop put 1st replica on node same as client. Client running out of cluster, node is chosen at random. 2nd replica at different rack. 3rd is different node but same rack as 2nd.\n",
    "![](images/replica.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce : Overview\n",
    "\n",
    "* Block size can be changed on cluster or even defined for particular file\n",
    "* A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster. If a task fails, it will be automatically rescheduled to run on a different node.\n",
    "\n",
    "* For data locality optimization, Hadoop try to run mapper on the same node where data resides. If no node is available for mapping task where the data is, job scheduler will look for another node in same rack. If even there no node are available then go for search in other rack.\n",
    "![](images/data_locality.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal split size for input for mapper is block size.\n",
    "* Output of map task is stored on local disk NOT on HDFS.\n",
    "* Reduce can NOT take benefit of data locality as multiple mapper sends their output to (single) reducer. Output of mappers are merged and passed to reducer. Output of reducer is stored on HDFS. For each HDFS block of reducer output 1st replica stored in local disk and others at off rack.\n",
    "![](images/map_reduce_flow.jpg)\n",
    "\n",
    "* Number of  reducer is NOT depends on input data, we have to specify it.\n",
    "* When there are multiple reducers, the map tasks partition their output, each creating one partition for each reduce task. There can be many keys (and their associated values) in each partition, but the records for any given key are all in a single partition. The partitioning can be controlled by a user-defined partitioning function, but normally the default partitioner—which buckets keys using a hash function—works very well.\n",
    "![](images/multiple_Reducer.jpg)\n",
    "\n",
    "\n",
    "* It is possible to have only mapper and 0 reducers\n",
    "![](images/mapper_no_reducer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiner\n",
    "* To reduce bandwidth usage between mapper and reducer, we can specify combiner function which will run on mapper node. Combiner's output goes to reducer.\n",
    "* Keep in mind, not always you can use combiner, think about mean. median problem.\n",
    "[Program to find Max temperature](https://github.com/purvil/hadoop_java/blob/master/max_temp/MaxTemp.java)\n",
    "* In Hadoop streaming we can define combiner with `-combiner combiner.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Streaming\n",
    "* Using this API we can write MR program in any language as long as it can read standard i/p and write to standard o/p.\n",
    "* Uses UNIX standard stream to communicate between program and hadoop.\n",
    "* Map input data is passed over standard ip to mapper. Which process it line by line and write it to standard output. Output is tab delimited line.\n",
    "* Typical java mapper, process input one at a time. Framework call Map() method of mapper for each record line in input. But in streaming entire input is given so mapper can decide how to process input. \n",
    "* In JAVA API we are given iterator for each key. In streaming we have to manually figure out boundary.\n",
    "* number of part files in a MapReduce program's output directory is same as number of reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Distributed File System (HDFS)\n",
    "* File system which manage storage  across network of machines called distributed FS.\n",
    "* Not good for low latency data access (use HBase). It is designed for high throughput data access. Read many write once principal.\n",
    "* Not good when there are lots of small files. As nomenade has to store all the metadata for file and it takes significant memory. 150 bytes per block is needed for metadata. Furthermore number of checks with datanodes proportional to number of blocks, which can trigger network load. Larger number of smaller blocks, we need map task per block. 10 GB of data, 32k is file size => 327680 map tasks. we can not have such number of nodes and so lots of queued tasks. Large overhead of spin up/tear down for each task. Inefficient disk I/O with small sizes. SOlution is merge/concate files. sequences files. combinefileinputformat.\n",
    "* Larger the file Hadoop spend less on seeking for the next data location. Seek are expensive operation.\n",
    "* Write is made to end of file, in append only fashion. No support for arbitrary offset writing or multiple writing.\n",
    "\n",
    "### Blocks\n",
    "* Disk has block size which is min amount of data it can read or write.\n",
    "* HDFS block are 128 MB by default.\n",
    "* file size less than block, does not hold full 128 MB.\n",
    "* Large HDFS block minimize seek time.\n",
    "* File can be larger than single disk when divided as blocks.\n",
    "* We can analyze disk usage easily, as we know the block size and will able to guess number of block in disk. Permission and other metadata of block are stored separately.\n",
    "* Replication provide availability and fault tolerance. Each block is replicated over several nodes. To match replication level, if node is failed. Block resides in that failed node are replicated one more time.\n",
    "* `hdfs fsck / -files -blocks` will list all blocks make up each file in file system. \n",
    "* Master-worker pattern: 2 kinds of nodes datanode and namenode. Namenode manage FS namespace. Maintains FS tree and metadata for all files and dir. On local disk it is stored as namespace image and edit log. It also knows where in which datanode block for files are stored. Block location is not in persistent storage as it can be reconstructed from data node on start.\n",
    "* Data node store and retrieve node when they are told to. Periodically report Name node about block they have.\n",
    "* If name node fails, we can not re generate file system\n",
    "    - First way to make persistent copy, like NFS.\n",
    "    - Secondary namenode: periodically merge image with edit log, to not to make edit log too large. It keeps copy of merged namespace image. But in the failure edit log is lost which is created after the large image creation. But we have backed up that on NFS. So, we will be fine.\n",
    "\n",
    "#### Block caching:\n",
    "* Block can be cached in data node's memory. By default each block can be cached in one data node. Application can tell namenode about which file to cache by adding cache directive to cache pool.\n",
    "\n",
    "#### HDFS federation\n",
    "* Allows multiple NameNodes with their own namespaces to share a pool of DataNodes.\n",
    "* When there are too many files in system. namenode's disk can be small to store all metadata. In that case we can deploy 2 namenode which acts independently and manages partial file system namespace. Each datanode connected with both namenode and stores block from any.\n",
    "* increased namespace scalability.\n",
    "* If some application has high metadata use we can isolate it in its own namespace.\n",
    "![](images/hadoop_federation.jpg)\n",
    "\n",
    "#### HDFS high availability\n",
    "* Combining NFS backup and storing checkpoint image of namenode can protect from data loss but does not increase availability. Namenode is the only machine to which we ask for file to block mapping.\n",
    "* To start another namenode after failure takes time for loading namespace image, replay edit log from NFS, receive enough block report from datanode.\n",
    "* To overcome in Haddop high availability mode there can be more than 1 active name node. If active fails, standalone take its responsibility.\n",
    "    - NameNodes has to use highly available shared storage to share the edit log. When standby namenode comes up it reads up to the end of edit log to synchronize the state with active name node.\n",
    "    - Datanodes must send block report to both as block mapping is stored in namenode's memory not disk.\n",
    "* If namenode fails, standby take it fast as latest state is available in memory.\n",
    "* In the case of fail we have to fence so that old name node which we think it is failed, does not handle any request. Fencing method can be block access to shared storage, turn off the power of namenode, disable network port.\n",
    "* Uses JournalNodes to decide the active NameNode. number of journal nodes are always odd number.\n",
    "\n",
    "#### Pseudo distributed mode\n",
    "* single node cluster. namenode and datanode on same machine.\n",
    "* set `fs.defaultFS = hdfs://localhost/` and `dfs.replication = 1` Default replication is 3.\n",
    "\n",
    "#### Command line interface\n",
    "* Also used below commands as `hdfd dfs`\n",
    "* `hadoop fs -copyFromLocal localpath hdfspath` // -put\n",
    "* `hadoop fs -copyToLocal hdfspath localspath` // -get\n",
    "* `hadoop fs -mkdir dir_name`\n",
    "* `hadoop fs -ls ` // returns replication factor too.\n",
    "* `hadoop fs -help`\n",
    "* `hadoop fs -usage <utility name>`\n",
    "* `hadoop fs -du -h` // disk usage human readable\n",
    "* `hadoop fs -df` consumed and available storage\n",
    "* `hadoop fs -mkdir -p /level/dir_name` // create deep nested dir\n",
    "* `hadoop fs -rm -r dir`\n",
    "* To strictly delete it use `-skipTrash` command.\n",
    "* `hadoop fs -touchz file.txt` // create 0 length file\n",
    "* `hadoop fsck filename -files -blocks` // total size, total files, symlinks, total blocks, minimally replicated clocks, overly replicated blocks, under replicated blocks, default replication factor, avg block replication, number of data nodes, number of racks.\n",
    "* `hadoop fsck -blockId blk_1073741825` // info about block using its id.\n",
    "* `hadoop fs -tail filename` // shows last 1 KB of HDFS file on stdout.\n",
    "* `hadoop fs -cat filename | head -4`\n",
    "* `hadoop fs -cat filename | tail -4`\n",
    "* `hadoopf fs -find /data/wiki -name '*part*'`\n",
    "* `hadoop fs -chmod`\n",
    "* `hadoop fdadmin -report` // overall report of cluster\n",
    "* All fs shell commands takes path URI as argument (Universal Resource identifier)\n",
    "    - `scheme://authority/path`\n",
    "    - scheme for hdfs is hdfs\n",
    "    - scheme for local fs is file\n",
    "* `hadoop fs -cp file:///sampleData/spark/myfile.txt hdfs://rvm.svl.ibm.com:8020/user/spark/text/.`\n",
    "* Default scheme and authority are in `core-site.xml`.\n",
    "* `getMerge`: Gets all files in directories that match the source pattern. Merges and sorts them to only one file on local fs.\n",
    "* `setRep` : sets replication factor of file. can specify to wait using `-w` until the replication level is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * HDFS is just one of the FS that is compatible with hadoop abstract FS implementation `org.apache.hadoop.FileSystem`. There are other several FS implementation.\n",
    "![](images/hadoop_fs.png)\n",
    "\n",
    "* Even we can access local files and run mr on that, to access local from hadoop\n",
    "* `hadoop fs -ls file:///`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coherency model\n",
    "* Data visibility of reads and write of a file. Once 1 block is written, that block is visible to new readers. Only current block being written is not visible to readers.\n",
    "* hflush() vs hsync()\n",
    "    - hflush() Force all buffers to be flushed to the datanodes. HDFS guarantees that the data written up to that point in the file has reached all the datanode.\n",
    "    - hflush() does not guarantee that data is written to disk. may be data only in memory of datanode. to gurantee that use hsync().\n",
    "    \n",
    "### distcp\n",
    "* Copying data to and from HDFS in parallel. Behind the scene it uses 20 mappers to accomplish task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `hdfs-site.xml` can be changed for, \n",
    "    - `dfs.blocksize` or `dfs.block.size` to change block size\n",
    "    - `dfs.replication`\n",
    "    - `dfs.datanode.handler.count(10)` : sets the number of server threads on each datanode\n",
    "    - `dfs.namenode.fs-limits.max-blocks-per-file` Max number of blocks per file\n",
    "* Namenode receives heartbeats from datanode. Datanode without recent heartbeat marked dead and no new IO sent. Blocks below replication factors re-replicated on other nodes.\n",
    "* Checksum  computed on file creation, checksum stored in hdfs namenode and used to check retrieved data. if failed re-read from alternate replica. Ex. CRC-32 (32 bit cyclic redundancy check) Other variant is CRC-32C\n",
    "* Separate checksum is created for every dfs.bytes-per-checksum bytes of data. Default is 512 bytes. Datanode varify checksum before storing data.\n",
    "* If client detect error in checksum it will notify namenode that particular replica is corrupted. namenode will re replicate it from good replica. \n",
    "* `hadoop fs -checksum filename` to find chcksum. also varify that 2 files are same?\n",
    "* Checksum provides DATA INTEGRITY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Original hadoop has issues like,\n",
    "    - Centralized handling of job control flow\n",
    "    - Tight coupling of a specific programming model with the resource management infrastructure.\n",
    "    - Scalability due  to budy jobtracker. Only q job tracker keeps track of all task trackers. and map reduce jobs. Whereas task tracker only has to do map and reduce task.\n",
    " * YARN split up 2 major functionality of JobTracker. Resource management and job scheduling/monitoring\n",
    "     - ResourceManager: Global resource manager and per node slave(nodemanager).Resourcemanager has complete power to assign resourced among all the applications in the system.  It has schedulter. \n",
    "     - ApplicationMaster: Per application application master is framework specific library and is tasked with negotiating  resources from resourcemanage and working with nodemanager to execute and manage task.\n",
    "* YARN is scalable higher than v1 because there is 1 application master per job which can run on any cluster node. Separation of functionality allows individual operation to be improved. Resourcemanager ONLY handle scheduling.\n",
    "* Multi-tenancy: YARN allows multiple access engines to use hadoop as the commin standard for batch, interactive and real time engine that can simultaneously access the same data set.\n",
    "* YARN uses shared pool of nodes for all jobs. Its like overloading in object orriented. every engine can use hadoop in different way.\n",
    "* YARN has high network utilization. Resource not used by one framework can be consumed by another.\n",
    "* Nodemanager is more generic than Tasktracker. TaskTracker has fixed number of map and reduce slot.  NodeManager has a number of dynamically create containers. Sized of container is amount of resources assigned to it like memory, CPU and network IO.\n",
    "* Resourcemanager keeps track of live nodemanagers and available resources. Allocate resources to applications and tasks. Monitor application master.\n",
    "* Nodemanager provides computation resources in form of container. Manage processes running in containers.\n",
    "* Application master coordinates execution of task within its application. Ask for resource containers to execute task.\n",
    "* Container can run different type of task, application master and are of different size.\n",
    "* Client can submit any type of application supported by YARN.\n",
    "![](images/YARN2.png)\n",
    "* Application connects to cluster and wants to start YARN job. Resource manager is contacted, resource manager launches application manager that will be responsible for this application(job). Application manager will ended when job will complete.\n",
    "* Application manager needs some resources, so it requests that from resourceManager. Resourcemanager replies with container id , which are allocated to this job. Applicationmanager works with container, to manage it.\n",
    "* Application master monitors mapper and reducer on each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN\n",
    "* Yet another resource negotiator.\n",
    "* Hadoop's cluster resource management system.\n",
    "* Provides API for requesting and working with cluster resources.\n",
    "* Many distributed computing framework can run on YARN and cluster storage layer.\n",
    "![](images/yarn.jpg)\n",
    "\n",
    "* YARN has resource manager (1 per cluster) to manage the use of resources across the cluster. And a node manager running on all nodes in the cluster to launch and monitor containers. Container execute application specific process with constrained set of resources.\n",
    "![](images/yarn1.jpg)\n",
    "* To run application on YARN client contact resource manager. Resource manager then finds a node manager that can launch the application master in a container.\n",
    "* Application master running in container can compute result and return to client or ask for more resources and run computation distributed.\n",
    "* Application can request set of containers which represent computer resources and also specify locality constraints.\n",
    "* Locality is expressed as same rack, specific node or anywhere in cluster.\n",
    "* Spark application request all resources upfront.\n",
    "* MapReduce on other hand, map task resources are requested upfront, reduce task resources are requested later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* YARN application's lifespan can be,\n",
    "    - 1 application per user job (MapReduce)\n",
    "    - 1 application per session of user. useful to cache intermediate result between jobs. (Spark)\n",
    "    - long running application like Impala daemon, which can be shared by different users. It has low latency so that request can be answered fast as no need of overhead to start application master.\n",
    "    \n",
    "![](images/mr1_YARN.jpg)\n",
    "* YARN's main goal is to separate resource management and job scheduling. There is 1 global resource manager. Node manager on each node. ApplicationMaster one for each application.\n",
    "* ApllicationMaster does job management and resource manager manages resources.\n",
    "* high availability resourcemanager : stand by resource manager\n",
    "\n",
    "#### YARN schedulers\n",
    "* FIFO, Capacity and fair schedulers.\n",
    "* Fairshare: balance resource share across the application over time\n",
    "* Capacity : Guaranteed capacity for each application\n",
    "![](images/fairshare.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data model\n",
    "* What data element are, what domain they come from, how different elements relate to each other, what they are composed of.\n",
    "* Relational data model. Table, row, column.\n",
    "* Graph data model: Vertices(entities) and edges (relation)\n",
    "* Data model defines structure of data.\n",
    "\n",
    "### File format\n",
    "* Defines physical data layout\n",
    "* csv, json, xls.\n",
    "* Defines how to transfer row bytes to programmable data structure and vice versa. (serialization and deserialization)\n",
    "* Space efficient, supported data types, encoding and decoding, splittable and monolithic structure (reading subset of data without reading entire file).\n",
    "\n",
    "#### CSV, TSV\n",
    "* bad space efficient\n",
    "* good encoding and decoding\n",
    "* only string is supported\n",
    "* splittable without header\n",
    "\n",
    "#### JSON\n",
    "* Java Script object notation\n",
    "* Not space efficient, includes field name\n",
    "* speedy encoding, decoding\n",
    "* supports maps, list, boolean, number, strings. Elements boiler plate serialization and deserialization code.\n",
    "\n",
    "#### XML\n",
    "\n",
    "### Binary format\n",
    "#### SequenceFile\n",
    "* Fist binary format in hadoop. Stores sequence of key and value. java serialization/deserialization\n",
    "\n",
    "#### Avro\n",
    "* Both format and support library\n",
    "* Stores object defined by schema, Specifies field name, type and aliasis. Support for language other than java\n",
    "\n",
    "#### RCFile (Record columnar)\n",
    "* Row based and column based format\n",
    "\n",
    "#### Parquet\n",
    "* Columnar format\n",
    "* Supports nested and repeated data\n",
    "\n",
    "### Compression\n",
    "* In CPU bound process compression is not useful. If program is IO bound use compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression\n",
    "* Saves disk usage and network bandwidth\n",
    "* -1 means optimized for speed, -9 means optimized for space.\n",
    "* `gzip -1 file`. gzip sits in between time/space trade offs. bzip2 compresses more effectively, but slower. LZO, LZ4 and snappy optimized for speed.\n",
    "* Splittable means we can seek to any point in stream and start reading from that point.\n",
    "* Codec is implementation of the compression and decompression algo.\n",
    "* If compression is not splittable then we can not create split for map task.\n",
    "* gzip use DEFLATE to store compressed data, DEFLATE stores data as series of compressed blocks and there is no way to distinguish start of each block. bzip2 support splitting (48 bit approximation of pi is used to show different blocks start point ).\n",
    "![](images/compression.jpg)\n",
    "![](images/compression1.jpg)\n",
    "\n",
    "```\n",
    "-D mapreduce.compress.map.output=true\n",
    "-D mapreduce.map.output.compression.codec=\n",
    "\n",
    "-D mapreduce.output.compress=true\n",
    "-D mapreduce.output.compression.codec=\n",
    "```\n",
    "* Container fiole format like sequenceFile, ORCFile, Avro support compression and splitting To use gzip, split the file in chunks and compress each.\n",
    "* Store very large file in splittable compressed format.\n",
    "* To store map reduce output in compressed form in job configuration se,\n",
    "```\n",
    "mapreduce.output.fileoutputformat.compress = true\n",
    "mapreduce.output.fileoutputformat.compress.codec = classname_compression_codec\n",
    "```\n",
    "* or set\n",
    "```\n",
    "FileOutputFormat.setCompressOutput(job, true);\n",
    "FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n",
    "```\n",
    "![](images/compression_property.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even intermidiate map data can be compressed, we can use fast compression method. \n",
    "![](images/map_compression.jpg)\n",
    "\n",
    "```\n",
    "Configuration conf = new Configuration();\n",
    "conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);\n",
    "conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class,\n",
    "CompressionCodec.class);\n",
    "Job job = new Job(conf);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "* Turning structured object into byte stream for transmission over a network. It is used in IPC and persistent storage.\n",
    "* It reduces the network bandwidth usage.\n",
    "* Hadoop use Writables as serialization format.\n",
    "\n",
    "### IntWritable:\n",
    "\n",
    "```\n",
    "IntWritable  writable = new IntWritable();\n",
    "writable.set(163);\n",
    "or\n",
    "IntWritable writable = new Intwritable(163);\n",
    "\n",
    "wrritable.get() // return 163\n",
    "```\n",
    "* IntWritable implements WritableComparable interface, which is subinterface of the Writable and java.lang.Comparable. This allows to compare them without deserializing. \n",
    "\n",
    "```\n",
    "RawComparator<IntWritable> comparator =\n",
    "WritableComparator.get(IntWritable.class);\n",
    "\n",
    "IntWritable w1 = new IntWritable(163);\n",
    "IntWritable w2 = new IntWritable(67);\n",
    "assertThat(comparator.compare(w1, w2), greaterThan(0));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/writable_type.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text \n",
    "* Writable for UTF-8 sequence.\n",
    "\n",
    "```\n",
    "Text t = new Text(\"hadoop\");\n",
    "t.getLength()\n",
    "t.charAt(2) // returns integer representing Unicode code point\n",
    "\n",
    "t.find(\"do\") // returns 2 same as indexOf()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map-reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mapper class is generic type, with 4 args of map function's\n",
    "    - input key type\n",
    "    - input val type\n",
    "    - output key type\n",
    "    - output val type\n",
    "    \n",
    "```\n",
    "public class MyMapper extends Mapper<LongWritable, Text, Text, IntWritable> // ip key, ip val, op key, op val type of map function\n",
    "{\n",
    "    // override map\n",
    "    public void map(Longwritable key, Text val, Context context) throws IOException\n",
    "    {\n",
    "        <logic>\n",
    "        context.write(new Text(key_str), new IntWriteable(val_int)); // collect output\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "* Instance of `Context` is used to write the output to.\n",
    "\n",
    "### Writable\n",
    "* In MR types are like IntWritable, Text.\n",
    "* Writable is an interface in Hadoop.\n",
    "* It acts as a wrapper to primitive data types of Java.\n",
    "* All MR data types must implement writable interface\n",
    "![](images/writable.jpg)\n",
    "* `org.apache.hadoop.io` has these types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce in detail\n",
    "* Components in hadoop are configured using Hadoop's own configuration API. An instance of configuration class (in org.apache.hadoop.conf package) represent config property and values.\n",
    "* It reads the property from config xml file.\n",
    "```\n",
    "<?xml version=\"1.0\"?>\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>color</name>\n",
    "            <value>yellow</value>\n",
    "            <description>Color</description>\n",
    "        </property>\n",
    "        <property>\n",
    "            <name>size</name>\n",
    "            <value>10</value>\n",
    "            <description>Size</description>\n",
    "        </property>\n",
    "        <property>\n",
    "            <name>weight</name>\n",
    "            <value>heavy</value>\n",
    "            <final>true</final>  // can not be overridden in subsequent definition\n",
    "            <description>Weight</description>\n",
    "        </property>\n",
    "        <property>\n",
    "            <name>size-weight</name>\n",
    "            <value>${size},${weight}</value> // config as other property, expanded using config.\n",
    "            <description>Size and weight</description>\n",
    "        </property>\n",
    "    </configuration>\n",
    "```\n",
    "* Above file  is saved as `config-1.xml`.\n",
    "\n",
    "```\n",
    "Configuration conf = new Configuration();\n",
    "conf.addResource(\"config-1.xml\");\n",
    "conf.get(\"color\");\n",
    "conf.getInt(\"size\", 0);\n",
    "conf.get(\"breadth\", \"wide\")' // defualt value is wide\n",
    "```\n",
    "* Overriding from command line,\n",
    "```\n",
    "-Dproperty=value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Config defined in resources that are added later override the earlier definitions.\n",
    "* For unit test we can use junit and to write mapreduce test mrunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can run Mapreduce job by calling submit() on Job object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map_reduce.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Client submit MR job. YARN resource manager, coordinate the allocation of computer resources on cluster. YARN node manager launch and monitor computer containers on machine in cluster. MR application master coordinates tasks running the MR job.MR master and MR task both run in containers that are scheduled by the resource manager and managed by node manager.\n",
    "* HDFS is used for sharing job files between other entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- submit() on job create internal JobSubmit instance and call submitJobInternal() on it. waitForCompletion() polls job's progress once per second and reports the progress to the console if something is changed.\n",
    "- JobSubmitter ask resource manager for new application ID, uses as MR job id.\n",
    "* Check output specification of job, if output die exist, error\n",
    "* Compute input splits for job. input path wrong,error.\n",
    "* Copies resource needed to run job, JAR file, config file, input split in shared FS to dir name after job id. (step 3)\n",
    "* Call submitApplication() on resource manager.\n",
    "\n",
    "* Resource manager hands on request to YARN scheduler. Scheduler allocates container and RM, launches application master there, under node manager's management.\n",
    "* Applications master is java application with main class MRAppMaster. It initialize job by number of jobkeeping expense by keeping track of job progress, it receive progress and completion report from task.Retrieve input split and create map task for each split and number of reducer job according to mapreduce.job.reduces (set by setNumReduceTasks() on Job) Each task is given id.\n",
    "* Application master decide how to run task, if job is small it will be run on same container as master to save container creation overhead. Such job is called uberized.\n",
    "* Small job is with less than 10 mappers and 1 reducers. Input size is less than 1 hdfs block. It can change by mapreduce.job.ubertask.maxmaps, mapreduce.job.ubertask.maxreduces, and map\n",
    "reduce.job.ubertask.maxbytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It will call setupJob() on OutputCommitter. FileOutputCommitter is default which create output directory and temp working place for task output.\n",
    "* IF uber job is not possible, then master request resource manager for containers for each map and reduce tasks. All map task must be completed before reduce can start so container for reduce task is requested when 5% of map finishes. Scheduler try to honor data loacality advantage to map task. Runninb on the same node where split resides, same rack.\n",
    "* Total memory, virtual cores for task is controlled by mapreduce.map.memory.mb, mapreduce.reduce.memory.mb, mapreduce.map.cpu, vcores and mapreduce.reduce.cpu.vcores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Master  starts container by contacting name node where container resides. Task will localize resources which it needs (job config and JAR, distributed cache)\n",
    "* Then it run map or reduce task.\n",
    "\n",
    "#### Streaming\n",
    "* Streaming runs special map and reduce task for the purpose of launching executable supplied by user.\n",
    "* Streaming task communicates with process using std ip and std op stream. Java process passes ip key-value pairs to external process. External process pass op key-value pair to Java process.\n",
    "![](images/streaming.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When master receives notification of last task that it is completed, master changes job status to successful.  Application master and task container clean up working state and intermediate output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Master will reschedule execution of failed task.\n",
    "* Task is failed when map or run throw runtime exception, streaming task exits with non zero exit code, or because of JVM bug. By default task is retried 4 times (controlled by mapreduce.map.maxattemots or  mapreduce.reduce.maxattempts). If task failed after 4 times whole job fails. We can set tolerance percentage of failed task using mapreduce.map.failures.maxpercent or mapreduce.reduce.failures.maxpercent\n",
    "* Speculative duplicate task are killed or when node manager is failed then master mark all tasks running on that node manager killed. Killed task are not counted towards attempted retry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even job are given retry attempts controlled by \"mapreduce.am.max-attempts\". default is 2. Also make sure yarn.resourcemanager.am.max-attempts are more or equal.\n",
    "* Master sends periodic heart beat to resource manager. In the event of application master failure, resource manager will detect and start new instance of master in new container.\n",
    "* Client poll master for update periodically, it has master's address cached given by resource manager in the beginning. If master is failed client has to locate new master instance. Client ask resource manager for it.\n",
    "* Node manager also send heart beat to resource manager. if no heart beat received in specified time, that node is removed from pool of available node.\n",
    "* All task is recovered from that name node and run on different node.\n",
    "* If on some name node task getting failed over and over that name node is black listed by master. Resource manager is NOT black listing so new job's task can be run on bad node.\n",
    "* Resource manager is single point of failure. to achieve high availability we can use active stand by resource manager. If active failed, the stand by will take responsibility.\n",
    "* stand by resource manager is activated by zookeeper (failover controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and sort\n",
    "* Input to every reducer is sorted by key.\n",
    "* Shuffle is the process by which system performs sort and transfer map output to reducers.\n",
    "![](images/shuffle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each map task has circular memory buffer that it writes output to. It is 100 MB by default (mapreduce.task.io.sort.mb) When content reaches to certain threshold size, background thread will start to spill the contents to disk. Spills are written to mapreduce.cluster.local.dir\n",
    "* Before written to disk, thread first divides data into partition corresponding to the reducers that will ultimately sent to. Within each partition background thread performs in memory sort by key.\n",
    "* Combiner function run on output of the sort. Running combiner makes more compact map output. less data written to disk and  transferred.\n",
    "* Each time buffer pass threshold new spill created.\n",
    "* Spill is merged in single partition and sorted.\n",
    "* If there are at least 3 spill files combiner will run again before output file is written.\n",
    "* Map op may also be compressed. (mapreduce.map.output.compress = true and mapreduce.map.output.compress.codec)\n",
    "* Map output is at local disk of machine where map ran. Now it is needed to the machine where reducer will run. Reduce node need map output for its partition from several map in cluster. Reduce copy each needed op from map to itself. By default 5 thread copy in parallel from various map.\n",
    " * Map output is deleted when master tell to do so to map after job is completed.\n",
    " * When in memory buffer reaches to threshold or threshold number of map output, it is merged and spilled to disk. If combiner is specified it will be run during the merge to reduce amount of data written to disk.\n",
    " * Background thread merges disk copies to large sorted file.\n",
    " * After all map op is copied reducer will move to sort phase, which merges the map op. If there are 50 files and merge factor is 10, there will be 5 rounds and at the end there will be 5 files, at the end there will be find merge to merge this 5 files. Final round of merge directs output to reducer.\n",
    "* Reduce function is called for each key in the sorted op. OP is written to op FS (HDFS) First replica is written to local disk.\n",
    "![](images/reducer_sort.jpg)\n",
    "* Give shuffle as much memory as possible\n",
    "* Make sure map and reduce use very low memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map_tuning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/reduce_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task execution environment\n",
    "![](images/task_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In streaming with python we can access it as `os.environ[\"mapreduce_job_id\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speculative execution\n",
    "* Man goal of MR model is to run task in parallel to reduce time than sequential run. There are straggling task which takes significantly longer time than others. Hadoop launch another equivalent task in backup. It is speculative execution of task. When task executed successfully, duplicate task will be killed.\n",
    "* Speculative execution is only useful when in original node/container there is hardware or software misconfiguration problem. If there is a problem in mapper/reducer it self, speculative execution will also run as slow as original task.\n",
    "![](images/speculative_execution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Speculative in reducer cause network traffic to transfer op of mapper.\n",
    "* Also not good  when cluster is busy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map_reduce_ip.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Partition function operates on intermediate key and value types and return partition index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map_reduce_config.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input type are set by input format, TextInputFormat generate keys of type LongWritable and values of type Text.\n",
    "* other type are set by calling methods on Job. If not set intermediate type are defaults to final output type, which defaults to LongWritable and Text.\n",
    "* If k2 and k3 are same no need to call setMapOutputKeyClass(), if v2 and v3 are same no need to call setMapOutputValueClass().\n",
    "\n",
    "#### Default MR job\n",
    "* Only config we set is ip and op path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data type which implement Writable interface is used as values. Implemented as WritableComparable interface is used as keys and values. As we need to compare keys during partitioning and sorting.\n",
    "![](images/writable_interface.jpg)\n",
    "* Why writable not primitive?\n",
    "    - During MR, data transfered from different nodes.  We need serialization and deserialization is done as hadoop only understand binary format, our data is structured data. It also decrease data size. So, writable is used to serialize and deserialize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "public class MyReducer extends Reducer<Text, IntWritable, Text, IntWritable>\n",
    "{\n",
    "    public void reduce(Text key, Iterator<IntWritable> val, Context obj) throws IOException\n",
    "    {\n",
    "        <logic>\n",
    "        obj.write(key, value); // make sure to convert to Writable\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of mapper\n",
    "1. Identity Mapper:\n",
    "    - Default. Same key value pair is output. Mapper<k,v,k,v>\n",
    "2. Inverse Mapper:\n",
    "    - Mapper<k,v,v,k>\n",
    "3. Regex Mapper - Provides a way to use regex in Map function\n",
    "    - Mapper<k, Text, Text, Longwritable>\n",
    "4. Token counter mapper\n",
    "    - Used to generate token count for key\n",
    "\n",
    "### Types of Reducer:\n",
    "1. Identity Reducer: Default reducer used to write the output same as input\n",
    "2. Long Sum Reducer: Determine sum of all values corresponding given key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver class\n",
    "- Telling hadoop which mapper and reducer class to use. Specify which partitioner and combiner and input format class to use.\n",
    "\n",
    "```\n",
    "public class MyDriver {\n",
    "public static void main(String[] args) throws exception {\n",
    "    if (args.length != 2) {\n",
    "        System.err.println(\"Usage is wrong\");\n",
    "        System.exit(-1);\n",
    "    }\n",
    "    \n",
    "    Job conf = new Job() // create job conf object\n",
    "    \n",
    "    conf.setJarByClass(MyDriver.class); // Hadoop will use it to locate jar file\n",
    "    conf.setJobName(\"Sample job name\") // set name of job\n",
    "    \n",
    "    FileInputFormat.addInputPath(conf, new Path(args[0])); // location from where mapper read ip\n",
    "    // We can specify such multiple path/directory\n",
    "    FileInputFormat.setOutputPath(conf, new Path(args[1])); // location where reducer write output\n",
    "    \n",
    "    conf.setMapperClass(MyMapper.class); // set mapper class\n",
    "    conf.setReducerClass(Reducer.class); // set reducer class\n",
    "    \n",
    "    conf.setMapOutputKeyClass(Text.class); // Set the output key type for the mapper\n",
    "    conf.setMapOutputValueClass(IntWritable.class); // Set the output Value type for the mapper\n",
    "    conf.setOutputKeyClass(Text.class); // // Set the output key type for the Reducer\n",
    "    conf.setOutputValueClass(IntWritable.class); // Set the output Value type for the Reducer\n",
    "    // If mapper class produce same output as reducer we can ommit setting types for mapper.\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class); // format of the input\n",
    "    conf.setOutputFormat(TextOutputFormat.class); // format of output\n",
    "    \n",
    "    System.exit(job.waitForCompletion(true)?0:1); \n",
    "}\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Job` obeject gives control and specification of how to run job.\n",
    "* We package our code in jar file, which is spreaded by hadoop around the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug MapReduce:\n",
    "* `cat testfile* | ./ your-mapper-program.py | sort | ./ your-reducer-program.py`\n",
    "* Hadoop has some environment variable that we can use such as task id.\n",
    "```\n",
    "import os\n",
    "myid = os.environ[\"mapred_tip_id\"]\n",
    "```\n",
    "* Open file to write\n",
    "```\n",
    "mylog = open(\"/tmp/mymaplog\"+myid, \"w\")\n",
    "mylog.write(debug_statement)\n",
    "mylog.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Data\n",
    "* Combining datasets by key\n",
    "![](images/join.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Result we want is,\n",
    "![](images/join_op.jpg)\n",
    "\n",
    "* Key is word and value is other info. File A already have Key as a word, in file B move date to value. In reducer there will be 1 line from file A and several from file B. Reducer has all the data at one place. So, it knows that from where the line is coming.\n",
    "![](images/join_merge.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector multiplication\n",
    "* Element of same index has to be grouped together and multiplied.\n",
    "* <key, value> = <index, number>\n",
    "* Bin the keys into ranges also reduce shuffling cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Cache:\n",
    "* Deliver require file to the nodes.\n",
    "* When we want to share the file across the node (container) where the mapper and reducer runs we can pass that with `-files` flag, meaning mapper.py, reducer.py will be shared/copied among all containers.\n",
    "* Other ways is to use `-archives` which utilize network and unpacked at worker node. Content of the tar file will be unpacked in the folder with the same name.\n",
    "* `-libjars` is used to distribute jars.\n",
    "\n",
    "### Environment and counters\n",
    "* Using distributed cache we can pass files to nodes.\n",
    "* Another way is to use job environment variables to get job configuration options.\n",
    "* When we launch job, hadoop assign input split of data to workers and we can get that data using,\n",
    "```\n",
    "if os.environ['mapred_task_is_map'] == \"true\": # I am mapper\n",
    "    os.environ[\"mapreduce_map_input_file\"]\n",
    "    os.environ[\"mapreduce_map_input_start\"]\n",
    "    os.environ[\"maprecuce_map_input_length\"]\n",
    "```\n",
    "* To access task id\n",
    "```\n",
    "os.environ[\"mapreduce_task_id\"] # absolute task id\n",
    "os.environ[\"mapreduce_task_parition\"]\n",
    "```\n",
    "\n",
    "* Using `-D` flag we can pass arbitrary environment variable\n",
    "```\n",
    "-D word_pattern=\"\\w+\\d+\"\n",
    "```\n",
    "* word_pattern can be accessed as environment variable.\n",
    "* `-mapper 'mapper.py --yout_param some_value` can also be used\n",
    "* All of above is communication from hadoop to python mapper/reducer\n",
    "* Other way communication is needed to see job progress. We can use status or counters\n",
    "```\n",
    "print(\"processed word\", file=sys.stderr) # status\n",
    "print(\"counte:msg Counters, word found, 1\", file=sys.stderr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "* Unit test using pytest\n",
    "* Integration testing like cat filename | ./mapper.py | sort | ./reducer.py\n",
    "* If mapper ore reducer file is using configuration options then use empty config. It is called system testing\n",
    "* Acceptance testing : use small testfiles and compare result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioner\n",
    "* Typically partitioner use 1st word (word before first \\t) as a key and sort it.\n",
    "* But what if I want to consider first 2 word as a key?\n",
    "```\n",
    "-D stream.num.map.output.key.fields=2\n",
    "```\n",
    "* Another options are\n",
    "```\n",
    "-D stream.map.output.field.separator=.\n",
    "```\n",
    "\n",
    "```\n",
    "-D mapreduce.partition.keypartitioner.options=-k1.2,1.2 # k1 is first word and .2 is second  \n",
    "                                                      #character. then 1.2 signify end character.\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedParitioner\n",
    "```\n",
    "\n",
    "### Comparator\n",
    "* Specify rule to show 1 key is bigger than other.\n",
    "\n",
    "```\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.parition.KeyFieldBasedComparator\n",
    "\n",
    "-D mapreduce.map.output.key.field.separator=.\n",
    "\n",
    "-D mapreduce.paritioner.keycomparator.options=\"-k2,3nr\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### File input format\n",
    "1. Text I/P format\n",
    "    - Default. Each line is record. Key is byte offset value is entire line.\n",
    "    ![](images/text_ip_format.jpg)\n",
    "2. Key value text input format\n",
    "    ![](images/key_val_ip.jpg)\n",
    "3. N-line input format\n",
    "    - Same as text ip format\n",
    "    - In text ip format we can not decide how many line processed by mapper, here we can set N to do that. default N = 1. Each mapper will process only 1 line.\n",
    "    ![](images/n_line_ip.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mapper, Reducer and Driver class are important\n",
    "\n",
    "## Mapper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Partitioner class\n",
    "* Class takes input from mapper and for each key creates list of values and pass to reducer. ALso decides which key will go to which reducers.\n",
    "\n",
    "```\n",
    "public abstract class Partitioner<KEY, VALUE>extends Object\n",
    "{\n",
    "    public abstract int getPartition(KEY k, VALUE v, int numPartitions) // Decide to which reducer particular key/val go\n",
    "    {\n",
    "    }\n",
    "}\n",
    "```\n",
    "* Abstract meaning any class which inherits Partitioner class has to provide implementation of all methods.\n",
    "![](images/default_partitioner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/partitioner_sort.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mapper store output to in memory buffer (default size is 100 MB can be changed by io.sort.mb). When map writes in buffer and threshold reaches (80% default), daemon will start writing content of buffer to disk (threshold can be set by io.sort.spill.percent.property). spills are written in mapred.local.dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data lakes are method of storing data that keep vast amount of data in their native format and horizontally to support the analysis of originally disparate source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
